{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a590b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, Input, LSTM, Reshape, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.layers import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2385e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lob_model(latent_dim, T):\n",
    "\tlob_state = keras.layers.Input(shape=(T, 40, 1))\n",
    "\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (1, 2), strides=(1, 2))(lob_state)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (1, 5), strides=(1, 5))(conv_first1)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (1, 4))(conv_first1)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\tconv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "\tconv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "\t# build the inception module\n",
    "\tconvsecond_1 = keras.layers.Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "\tconvsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\tconvsecond_1 = keras.layers.Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
    "\tconvsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "\tconvsecond_2 = keras.layers.Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "\tconvsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\tconvsecond_2 = keras.layers.Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
    "\tconvsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "\tconvsecond_3 = keras.layers.MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "\tconvsecond_3 = keras.layers.Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
    "\tconvsecond_3 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "\n",
    "\tconvsecond_output = keras.layers.concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "\tconv_reshape = keras.layers.Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(\n",
    "\t\tconvsecond_output)\n",
    "\n",
    "\tattn_input = conv_reshape\n",
    "\tattn_input_last = attn_input[:, -1:, :]\n",
    "\n",
    "\tmulti_head_attn_layer_1 = MultiHeadAttention(num_heads=10, key_dim=16, output_shape=64)\n",
    "\n",
    "\tattn_output, weight = multi_head_attn_layer_1(attn_input_last, attn_input, return_attention_scores=True)\n",
    "\n",
    "\tattn_output = keras.layers.Flatten()(attn_output)\n",
    "\n",
    "\t# add Batch Normalization\n",
    "\t# attn_output = keras.layers.BatchNormalization()(attn_output)\n",
    "\n",
    "\t# add Layer Normalization\n",
    "\t# attn_output = keras.layers.LayerNormalization()(attn_output)\n",
    "\n",
    "\treturn keras.models.Model(lob_state, attn_output)\n",
    "\n",
    "def getLabel(mid_price, horizon, threshold=1e-5):\n",
    "    price_past = mid_price.rolling(window=horizon).mean()\n",
    "\n",
    "    price_future = mid_price.copy()\n",
    "    price_future[:-horizon] = price_past[horizon:]\n",
    "    price_future[-horizon:] = np.nan\n",
    "\n",
    "    pct_change = (price_future - price_past)/price_past\n",
    "    pct_change[pct_change>=threshold] = 1\n",
    "    pct_change[(pct_change<threshold) & (-threshold<pct_change)] = 2\n",
    "    pct_change[pct_change<=-threshold] = 3\n",
    "    return pct_change\n",
    "\n",
    "def process_data(data):\n",
    "    #data = data[(data.time > '10:00:00')&(data.time < '14:30:00')]\n",
    "    data = data.dropna()\n",
    "    data.y = getLabel(data.midprice, 10)\n",
    "\n",
    "    for i in range(10):\n",
    "        data[f'ask_price_{i+1}'] = data[f'ask_price_{i+1}']/data['midprice'] - 1\n",
    "        data[f'bid_price_{i+1}'] = data[f'bid_price_{i+1}']/data['midprice'] - 1\n",
    "        data[f'ask_quantity_{i+1}'] = data[f'ask_quantity_{i+1}']/data[f'ask_quantity_{i+1}'].max()\n",
    "        data[f'bid_quantity_{i+1}'] = data[f'bid_quantity_{i+1}']/data[f'bid_quantity_{i+1}'].max()\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "    return data.set_index(['timestamp'])\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "\t[N, D] = X.shape\n",
    "\tdf = np.array(X)\n",
    "\tdY = np.array(Y)\n",
    "\tdataY = to_categorical(dY[T-1:N], 3)\n",
    "\tdataX = np.zeros((N-T+1, T, D))\n",
    "\tfor i in range(T, N+1):\n",
    "\t\tdataX[i-T] = df[i-T:i, :]\n",
    "\treturn dataX.reshape(dataX.shape+(1,)), dataY\n",
    "\n",
    "def prepare_data(file_path):\n",
    "\tdf = pd.read_csv(file_path)\n",
    "\tdf['midprice'] = 0.5 * (df['ask_price_1'] + df['bid_price_1'])\n",
    "\tdf = df.dropna()\n",
    "\n",
    "\tdf['y'] = getLabel(df.midprice, 10)\n",
    "\tdf['y'] = df['y'] - 1\n",
    "\n",
    "\tfor i in range(10):\n",
    "\t\tdf[f'ask_price_{i+1}'] = df[f'ask_price_{i+1}'] / df['midprice'] - 1\n",
    "\t\tdf[f'bid_price_{i+1}'] = df[f'bid_price_{i+1}'] / df['midprice'] - 1\n",
    "\t\tdf[f'ask_quantity_{i + 1}'] = df[f'ask_quantity_{i + 1}'] / df[f'ask_quantity_{i + 1}'].max()\n",
    "\t\tdf[f'bid_quantity_{i + 1}'] = df[f'bid_quantity_{i + 1}'] / df[f'bid_quantity_{i + 1}'].max()\n",
    "\tdf.set_index('timestamp', inplace=True)\n",
    "\n",
    "\tall_columns = []\n",
    "\tfor index in range(1,11):\n",
    "\t\tall_columns.append(f\"ask_quantity_{index}\")\n",
    "\t\tall_columns.append(f\"ask_price_{index}\")\n",
    "\t\tall_columns.append(f\"bid_quantity_{index}\")\n",
    "\t\tall_columns.append(f\"bid_price_{index}\")\n",
    "\tall_columns.append('y')\n",
    "\tdf = df[all_columns]\n",
    "\tdf = df.dropna()\n",
    "\treturn np.array(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eab3d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = prepare_data(r'data/600519/20230320/order_book.csv')\n",
    "df_val = prepare_data(r'data/600519/20230321/order_book.csv')\n",
    "df_test = prepare_data(r'data/600519/20230322/order_book.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1103a19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48595, 50, 40, 1) (48595, 3)\n",
      "(75751, 50, 40, 1) (75751, 3)\n",
      "(43013, 50, 40, 1) (43013, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "T=50\n",
    "trainX, trainY = data_classification(df_train[:-10,:-1], df_train[:-10, -1:], T)\n",
    "valX, valY = data_classification(df_val[:-10,:-1], df_val[:-10, -1:], T)\n",
    "testX, testY = data_classification(df_test[:-10,:-1], df_test[:-10, -1:], T)\n",
    "\n",
    "print(trainX.shape, trainY.shape)\n",
    "print(valX.shape, valY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea4a6a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50, 40, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 50, 20, 32)   96          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 50, 20, 32)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 50, 20, 32)   4128        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 50, 20, 32)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 50, 20, 32)   4128        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 50, 20, 32)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 50, 4, 32)    5152        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 50, 4, 32)    0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 50, 4, 32)    4128        ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 50, 4, 32)    0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 50, 4, 32)    4128        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 50, 4, 32)    0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 50, 1, 32)    4128        ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 50, 1, 32)    0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 50, 1, 32)    4128        ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 50, 1, 32)    0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 50, 1, 32)    4128        ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 50, 1, 32)    0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 50, 1, 64)    2112        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 50, 1, 64)    2112        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 50, 1, 64)    0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, 50, 1, 64)    0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 50, 1, 32)    0           ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 50, 1, 64)    12352       ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 50, 1, 64)    20544       ['leaky_re_lu_11[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 50, 1, 64)    2112        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 50, 1, 64)    0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 50, 1, 64)    0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 50, 1, 64)    0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 50, 1, 192)   0           ['leaky_re_lu_10[0][0]',         \n",
      "                                                                  'leaky_re_lu_12[0][0]',         \n",
      "                                                                  'leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 50, 192)      0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1, 192)      0           ['reshape[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  ((None, 1, 64),     102944      ['tf.__operators__.getitem[0][0]'\n",
      " dAttention)                     (None, 10, 1, 50))              , 'reshape[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 176,320\n",
      "Trainable params: 176,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 40, 1)]       0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model (Functional)          (None, 64)                176320    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 176,515\n",
      "Trainable params: 176,515\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "checkpoint_filepath = 'model_tensorflow2/weights_china'\n",
    "\n",
    "lob_model = get_lob_model(64, T)\n",
    "lob_model.summary()\n",
    "\n",
    "def get_pretrain_model(model, T):\n",
    "    lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
    "    embedding = model(lob_state)\n",
    "    output = keras.layers.Dense(3, activation='softmax')(embedding)\n",
    "\n",
    "    return keras.models.Model(lob_state, output)\n",
    "\n",
    "china_model = get_pretrain_model(lob_model, T)\n",
    "china_model.summary()\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "\tfilepath = checkpoint_filepath,\n",
    "\tsave_weights_only = True,\n",
    "\tmonitor = 'val_loss',\n",
    "\tmode = 'auto',\n",
    "\tsave_best_only = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d65c4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "760/760 - 22s - loss: 0.9866 - accuracy: 0.4862 - val_loss: 1.1340 - val_accuracy: 0.3842 - 22s/epoch - 28ms/step\n",
      "Epoch 2/30\n",
      "760/760 - 19s - loss: 0.9778 - accuracy: 0.4893 - val_loss: 1.1422 - val_accuracy: 0.4020 - 19s/epoch - 25ms/step\n",
      "Epoch 3/30\n",
      "760/760 - 21s - loss: 0.9647 - accuracy: 0.4999 - val_loss: 1.2176 - val_accuracy: 0.3633 - 21s/epoch - 28ms/step\n",
      "Epoch 4/30\n",
      "760/760 - 19s - loss: 0.9555 - accuracy: 0.5088 - val_loss: 1.2108 - val_accuracy: 0.3859 - 19s/epoch - 25ms/step\n",
      "Epoch 5/30\n",
      "760/760 - 19s - loss: 0.9448 - accuracy: 0.5164 - val_loss: 1.2804 - val_accuracy: 0.3948 - 19s/epoch - 25ms/step\n",
      "Epoch 6/30\n",
      "760/760 - 19s - loss: 0.9349 - accuracy: 0.5226 - val_loss: 1.2471 - val_accuracy: 0.3821 - 19s/epoch - 25ms/step\n",
      "Epoch 7/30\n",
      "760/760 - 19s - loss: 0.9248 - accuracy: 0.5283 - val_loss: 1.3316 - val_accuracy: 0.3662 - 19s/epoch - 25ms/step\n",
      "Epoch 8/30\n",
      "760/760 - 19s - loss: 0.9144 - accuracy: 0.5376 - val_loss: 1.2752 - val_accuracy: 0.3764 - 19s/epoch - 25ms/step\n",
      "Epoch 9/30\n",
      "760/760 - 19s - loss: 0.9061 - accuracy: 0.5426 - val_loss: 1.3307 - val_accuracy: 0.3687 - 19s/epoch - 25ms/step\n",
      "Epoch 10/30\n",
      "760/760 - 19s - loss: 0.8956 - accuracy: 0.5485 - val_loss: 1.5049 - val_accuracy: 0.3880 - 19s/epoch - 25ms/step\n",
      "Epoch 11/30\n",
      "760/760 - 19s - loss: 0.8860 - accuracy: 0.5557 - val_loss: 1.4517 - val_accuracy: 0.3709 - 19s/epoch - 25ms/step\n",
      "Epoch 12/30\n",
      "760/760 - 19s - loss: 0.8772 - accuracy: 0.5674 - val_loss: 1.4966 - val_accuracy: 0.3735 - 19s/epoch - 25ms/step\n",
      "Epoch 13/30\n",
      "760/760 - 21s - loss: 0.8650 - accuracy: 0.5740 - val_loss: 1.6538 - val_accuracy: 0.3859 - 21s/epoch - 28ms/step\n",
      "Epoch 14/30\n",
      "760/760 - 19s - loss: 0.8559 - accuracy: 0.5820 - val_loss: 1.7717 - val_accuracy: 0.3834 - 19s/epoch - 25ms/step\n",
      "Epoch 15/30\n",
      "760/760 - 19s - loss: 0.8440 - accuracy: 0.5915 - val_loss: 1.6844 - val_accuracy: 0.3796 - 19s/epoch - 25ms/step\n",
      "Epoch 16/30\n",
      "760/760 - 19s - loss: 0.8312 - accuracy: 0.5964 - val_loss: 1.8259 - val_accuracy: 0.3853 - 19s/epoch - 25ms/step\n",
      "Epoch 17/30\n",
      "760/760 - 19s - loss: 0.8209 - accuracy: 0.6073 - val_loss: 1.6897 - val_accuracy: 0.3789 - 19s/epoch - 25ms/step\n",
      "Epoch 18/30\n",
      "760/760 - 19s - loss: 0.8107 - accuracy: 0.6159 - val_loss: 1.8178 - val_accuracy: 0.3791 - 19s/epoch - 25ms/step\n",
      "Epoch 19/30\n",
      "760/760 - 19s - loss: 0.7976 - accuracy: 0.6249 - val_loss: 1.8518 - val_accuracy: 0.3781 - 19s/epoch - 25ms/step\n",
      "Epoch 20/30\n",
      "760/760 - 19s - loss: 0.7883 - accuracy: 0.6319 - val_loss: 1.9325 - val_accuracy: 0.3778 - 19s/epoch - 25ms/step\n",
      "Epoch 21/30\n",
      "760/760 - 19s - loss: 0.7781 - accuracy: 0.6379 - val_loss: 1.9225 - val_accuracy: 0.3833 - 19s/epoch - 25ms/step\n",
      "Epoch 22/30\n",
      "760/760 - 19s - loss: 0.7712 - accuracy: 0.6417 - val_loss: 1.9444 - val_accuracy: 0.3747 - 19s/epoch - 25ms/step\n",
      "Epoch 23/30\n",
      "760/760 - 19s - loss: 0.7583 - accuracy: 0.6491 - val_loss: 1.9872 - val_accuracy: 0.3766 - 19s/epoch - 25ms/step\n",
      "Epoch 24/30\n",
      "760/760 - 19s - loss: 0.7505 - accuracy: 0.6547 - val_loss: 2.0575 - val_accuracy: 0.3845 - 19s/epoch - 25ms/step\n",
      "Epoch 25/30\n",
      "760/760 - 19s - loss: 0.7392 - accuracy: 0.6634 - val_loss: 1.9635 - val_accuracy: 0.3695 - 19s/epoch - 25ms/step\n",
      "Epoch 26/30\n",
      "760/760 - 19s - loss: 0.7368 - accuracy: 0.6618 - val_loss: 2.1283 - val_accuracy: 0.3855 - 19s/epoch - 25ms/step\n",
      "Epoch 27/30\n",
      "760/760 - 19s - loss: 0.7241 - accuracy: 0.6689 - val_loss: 2.1170 - val_accuracy: 0.3741 - 19s/epoch - 25ms/step\n",
      "Epoch 28/30\n",
      "760/760 - 19s - loss: 0.7159 - accuracy: 0.6739 - val_loss: 2.2172 - val_accuracy: 0.3842 - 19s/epoch - 25ms/step\n",
      "Epoch 29/30\n",
      "760/760 - 19s - loss: 0.7120 - accuracy: 0.6780 - val_loss: 2.2456 - val_accuracy: 0.3875 - 19s/epoch - 25ms/step\n",
      "Epoch 30/30\n",
      "760/760 - 19s - loss: 0.7009 - accuracy: 0.6837 - val_loss: 2.2116 - val_accuracy: 0.3753 - 19s/epoch - 25ms/step\n"
     ]
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "china_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "china_model.fit(trainX, trainY, validation_data=(valX, valY), epochs=30, batch_size=64, verbose=2, callbacks=[model_checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51b2f5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/userhome/cs/dengcfei/MarketMakingViaReinforcementLearning'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b64562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "china_model.save_weights(r\"model_tensorflow2/china_model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a44ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = china_model.history.history['accuracy']\n",
    "val_acc = china_model.history.history['val_accuracy']\n",
    "loss = china_model.history.history['loss']\n",
    "val_loss = china_model.history.history['val_loss']\n",
    "epochs = range(1, len(acc)+1 )\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title(\"Training and vlaidation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "pred = china_model.predict(testX)\n",
    "print('test accuracy: ', accuracy_score(np.argmax(testX, axis=1), np.argmax(pred, axis=1)))\n",
    "print(classification_report(np.argmax(testY, axis=1), np.argmax(pred, axis=1), digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
